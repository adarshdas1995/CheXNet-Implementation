{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PvWONVSVTpp6",
    "outputId": "b1e4bdbe-2520-4482-cd6c-33b5ba44b289"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "l7kj2Wh3Tpp_",
    "outputId": "0f1f271a-93b2-4f1d-84e8-a6c8c7c33df8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# The Model\n",
    "input_img = Input(shape=(64, 64, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "# Conv Layer 1\n",
    "x = Conv2D(64, (3,3), padding = 'same')(input_img)\n",
    "x = BatchNormalization(axis = 3)(x)\n",
    "x = Activation('relu')(x)\n",
    "# Conv Layer 2\n",
    "x = Conv2D(32, (3,3), padding = 'same')(x)\n",
    "x = Conv2D(32, (3,3), padding = 'same')(x)\n",
    "x = Conv2D(32, (3,3), padding = 'same')(x)\n",
    "x = BatchNormalization(axis = 3)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size = (2,2)) (x)\n",
    "# Conv Layer 1\n",
    "x = Conv2D(64, (3,3), padding = 'same')(x)\n",
    "x = Conv2D(64, (3,3), padding = 'same')(x)\n",
    "x = Conv2D(64, (3,3), padding = 'same')(x)\n",
    "x = BatchNormalization(axis = 3)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size = (2,2)) (x)\n",
    "\n",
    "# Conv Layer 1\n",
    "x = Conv2D(32, (3,3), padding = 'same')(x)\n",
    "x = Conv2D(32, (3,3), padding = 'same')(x)\n",
    "x = Conv2D(32, (3,3), padding = 'same')(x)\n",
    "x = BatchNormalization(axis = 3)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size = (2,2)) (x)\n",
    "\n",
    "\n",
    "# Dense\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation = 'relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(512, activation = 'relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(512, activation = 'relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "outputs = Dense(2, activation='softmax')(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cnn_classifier = Model(input_img, outputs)\n",
    "#adam = optimizers.Adam(lr=0.0003, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#sgd = optimizers.SGD(lr=0.001, decay=1e-4, momentum=0.8, nesterov=True)\n",
    "cnn_classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1200
    },
    "colab_type": "code",
    "id": "X9Hn5dslTpqC",
    "outputId": "139b8279-530e-455a-d045-4308865d6e32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 3,185,154\n",
      "Trainable params: 3,184,770\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "6DpZ0anwUUhI",
    "outputId": "ce77ded0-5428-4fba-aac0-69bfdf8c82d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Cqmq876SUolk",
    "outputId": "672f12ac-acf4-4752-9d17-196d2503fd1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_data.npy\tUntitled0.ipynb  VAE_CHEXNET.ipynb\n",
      "train_v2.csv\tUntitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls \"/content/gdrive/My Drive/Chexnet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "18-AzRpFTpqF",
    "outputId": "f82318b9-717a-4fe2-9c9b-e937c16ad6c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26684, 4096)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/content/gdrive/My Drive/Chexnet/image_data.npy')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qpsg4U9FTpqI"
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('train_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGVq42GyTpqK"
   },
   "outputs": [],
   "source": [
    "x_train = data[:15000,:]\n",
    "x_validation = data[15000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iobOfykTTpqM",
    "outputId": "5c2f6a53-3c4b-4b28-c239-1669e3ceedbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 4096)\n",
      "(11684, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hULoD0qgTpqP"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_validation = x_validation.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 64, 64, 1))  # adapt this if using `channels_first` image data format\n",
    "x_validation = np.reshape(x_validation, (len(x_validation), 64, 64, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m596xQiroXey"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# Data Generator Module\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NxuTnb-3f5TD"
   },
   "outputs": [],
   "source": [
    "import keras.utils\n",
    "df = pd.read_csv('/content/gdrive/My Drive/Chexnet/train_v2.csv')\n",
    "labels_train = df['Target'][:15000]\n",
    "labels_test = df['Target'][15000:]\n",
    "labels_train = keras.utils.to_categorical(labels_train)\n",
    "labels_test = keras.utils.to_categorical(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYOTWd-SjbCS"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "lr_reducer = ReduceLROnPlateau(monitor='loss',factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "callbacks = [lr_scheduler, lr_reducer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10374
    },
    "colab_type": "code",
    "id": "r69-npu7TpqW",
    "outputId": "f7402c9d-f35e-4a09-d073-43d85b272a14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 19s 165ms/step - loss: 0.5761 - acc: 0.7708 - val_loss: 0.6664 - val_acc: 0.7655\n",
      "Epoch 2/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 127ms/step - loss: 0.4777 - acc: 0.7862 - val_loss: 0.9317 - val_acc: 0.2349\n",
      "Epoch 3/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 127ms/step - loss: 0.4548 - acc: 0.7946 - val_loss: 0.9474 - val_acc: 0.2345\n",
      "Epoch 4/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.4415 - acc: 0.7973 - val_loss: 0.8777 - val_acc: 0.2345\n",
      "Epoch 5/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.4376 - acc: 0.8036 - val_loss: 0.8516 - val_acc: 0.2345\n",
      "Epoch 6/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 124ms/step - loss: 0.4329 - acc: 0.8058 - val_loss: 0.5242 - val_acc: 0.7656\n",
      "Epoch 7/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 124ms/step - loss: 0.4320 - acc: 0.8067 - val_loss: 0.7082 - val_acc: 0.2656\n",
      "Epoch 8/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4244 - acc: 0.8095 - val_loss: 0.9195 - val_acc: 0.2346\n",
      "Epoch 9/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 127ms/step - loss: 0.4219 - acc: 0.8110 - val_loss: 1.5090 - val_acc: 0.2345\n",
      "Epoch 10/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4220 - acc: 0.8099 - val_loss: 0.6538 - val_acc: 0.6790\n",
      "Epoch 11/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4140 - acc: 0.8123 - val_loss: 1.2429 - val_acc: 0.2345\n",
      "Epoch 12/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 124ms/step - loss: 0.4188 - acc: 0.8095 - val_loss: 0.6516 - val_acc: 0.6826\n",
      "Epoch 13/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4163 - acc: 0.8142 - val_loss: 0.6202 - val_acc: 0.7528\n",
      "Epoch 14/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.4177 - acc: 0.8117 - val_loss: 0.6962 - val_acc: 0.5444\n",
      "Epoch 15/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4147 - acc: 0.8122 - val_loss: 0.7950 - val_acc: 0.2740\n",
      "Epoch 16/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4111 - acc: 0.8157 - val_loss: 0.9240 - val_acc: 0.2346\n",
      "Epoch 17/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4117 - acc: 0.8127 - val_loss: 0.6096 - val_acc: 0.7520\n",
      "Epoch 18/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4052 - acc: 0.8150 - val_loss: 0.5965 - val_acc: 0.7614\n",
      "Epoch 19/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4104 - acc: 0.8129 - val_loss: 0.7837 - val_acc: 0.3142\n",
      "Epoch 20/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.4017 - acc: 0.8186 - val_loss: 0.8197 - val_acc: 0.2591\n",
      "Epoch 21/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4050 - acc: 0.8178 - val_loss: 0.7785 - val_acc: 0.2396\n",
      "Epoch 22/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4080 - acc: 0.8157 - val_loss: 0.5273 - val_acc: 0.7655\n",
      "Epoch 23/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4072 - acc: 0.8165 - val_loss: 0.7370 - val_acc: 0.4345\n",
      "Epoch 24/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4048 - acc: 0.8170 - val_loss: 0.6713 - val_acc: 0.6734\n",
      "Epoch 25/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.4032 - acc: 0.8195 - val_loss: 0.6222 - val_acc: 0.6917\n",
      "Epoch 26/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 127ms/step - loss: 0.4022 - acc: 0.8168 - val_loss: 0.5968 - val_acc: 0.7566\n",
      "Epoch 27/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.4035 - acc: 0.8194 - val_loss: 0.5962 - val_acc: 0.7688\n",
      "Epoch 28/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3996 - acc: 0.8198 - val_loss: 0.6892 - val_acc: 0.6071\n",
      "Epoch 29/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4005 - acc: 0.8187 - val_loss: 0.5685 - val_acc: 0.7636\n",
      "Epoch 30/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4004 - acc: 0.8218 - val_loss: 0.5803 - val_acc: 0.7654\n",
      "Epoch 31/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3976 - acc: 0.8184 - val_loss: 0.5913 - val_acc: 0.7470\n",
      "Epoch 32/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3990 - acc: 0.8200 - val_loss: 0.5109 - val_acc: 0.7659\n",
      "Epoch 33/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3996 - acc: 0.8169 - val_loss: 0.5702 - val_acc: 0.7645\n",
      "Epoch 34/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3979 - acc: 0.8204 - val_loss: 0.6469 - val_acc: 0.7257\n",
      "Epoch 35/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.4005 - acc: 0.8191 - val_loss: 0.5851 - val_acc: 0.7678\n",
      "Epoch 36/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3966 - acc: 0.8208 - val_loss: 0.6347 - val_acc: 0.7629\n",
      "Epoch 37/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3983 - acc: 0.8170 - val_loss: 0.7576 - val_acc: 0.2771\n",
      "Epoch 38/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3941 - acc: 0.8221 - val_loss: 0.5795 - val_acc: 0.7600\n",
      "Epoch 39/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3931 - acc: 0.8212 - val_loss: 0.5710 - val_acc: 0.7633\n",
      "Epoch 40/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3961 - acc: 0.8189 - val_loss: 0.5348 - val_acc: 0.7666\n",
      "Epoch 41/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3967 - acc: 0.8180 - val_loss: 0.6153 - val_acc: 0.7542\n",
      "Epoch 42/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3949 - acc: 0.8197 - val_loss: 0.6900 - val_acc: 0.6498\n",
      "Epoch 43/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3928 - acc: 0.8158 - val_loss: 0.6215 - val_acc: 0.7366\n",
      "Epoch 44/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3951 - acc: 0.8249 - val_loss: 0.5541 - val_acc: 0.7653\n",
      "Epoch 45/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3924 - acc: 0.8207 - val_loss: 0.7637 - val_acc: 0.4710\n",
      "Epoch 46/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3942 - acc: 0.8247 - val_loss: 0.5537 - val_acc: 0.7657\n",
      "Epoch 47/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 127ms/step - loss: 0.3919 - acc: 0.8237 - val_loss: 0.5718 - val_acc: 0.7674\n",
      "Epoch 48/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3921 - acc: 0.8209 - val_loss: 0.7269 - val_acc: 0.4653\n",
      "Epoch 49/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3910 - acc: 0.8222 - val_loss: 0.6530 - val_acc: 0.7002\n",
      "Epoch 50/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3923 - acc: 0.8218 - val_loss: 0.5467 - val_acc: 0.7671\n",
      "Epoch 51/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3893 - acc: 0.8231 - val_loss: 0.5873 - val_acc: 0.7626\n",
      "Epoch 52/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3867 - acc: 0.8233 - val_loss: 0.6282 - val_acc: 0.7444\n",
      "Epoch 53/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3909 - acc: 0.8227 - val_loss: 0.5723 - val_acc: 0.7663\n",
      "Epoch 54/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3928 - acc: 0.8201 - val_loss: 0.6266 - val_acc: 0.7515\n",
      "Epoch 55/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3944 - acc: 0.8211 - val_loss: 0.5338 - val_acc: 0.7671\n",
      "Epoch 56/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3902 - acc: 0.8254 - val_loss: 0.6081 - val_acc: 0.7585\n",
      "Epoch 57/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3934 - acc: 0.8228 - val_loss: 0.6337 - val_acc: 0.7464\n",
      "Epoch 58/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3882 - acc: 0.8216 - val_loss: 0.5408 - val_acc: 0.7672\n",
      "Epoch 59/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3878 - acc: 0.8256 - val_loss: 0.5141 - val_acc: 0.7664\n",
      "Epoch 60/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3879 - acc: 0.8219 - val_loss: 0.5839 - val_acc: 0.7702\n",
      "Epoch 61/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3861 - acc: 0.8222 - val_loss: 0.5971 - val_acc: 0.7672\n",
      "Epoch 62/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3876 - acc: 0.8240 - val_loss: 0.5563 - val_acc: 0.7694\n",
      "Epoch 63/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3869 - acc: 0.8254 - val_loss: 0.5842 - val_acc: 0.7693\n",
      "Epoch 64/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3831 - acc: 0.8253 - val_loss: 0.6195 - val_acc: 0.7479\n",
      "Epoch 65/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3873 - acc: 0.8229 - val_loss: 0.5425 - val_acc: 0.7679\n",
      "Epoch 66/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3887 - acc: 0.8259 - val_loss: 0.5529 - val_acc: 0.7657\n",
      "Epoch 67/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3874 - acc: 0.8270 - val_loss: 0.6765 - val_acc: 0.6189\n",
      "Epoch 68/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3855 - acc: 0.8253 - val_loss: 0.6092 - val_acc: 0.7572\n",
      "Epoch 69/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3813 - acc: 0.8252 - val_loss: 0.5258 - val_acc: 0.7677\n",
      "Epoch 70/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3811 - acc: 0.8273 - val_loss: 0.5660 - val_acc: 0.7664\n",
      "Epoch 71/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3850 - acc: 0.8279 - val_loss: 0.5654 - val_acc: 0.7682\n",
      "Epoch 72/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3860 - acc: 0.8259 - val_loss: 0.5544 - val_acc: 0.7654\n",
      "Epoch 73/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3822 - acc: 0.8253 - val_loss: 0.6159 - val_acc: 0.7372\n",
      "Epoch 74/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3830 - acc: 0.8259 - val_loss: 0.5460 - val_acc: 0.7699\n",
      "Epoch 75/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3847 - acc: 0.8233 - val_loss: 0.5492 - val_acc: 0.7690\n",
      "Epoch 76/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3824 - acc: 0.8273 - val_loss: 0.6006 - val_acc: 0.7631\n",
      "Epoch 77/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3813 - acc: 0.8271 - val_loss: 0.6362 - val_acc: 0.7479\n",
      "Epoch 78/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3840 - acc: 0.8275 - val_loss: 0.6080 - val_acc: 0.7559\n",
      "Epoch 79/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3837 - acc: 0.8287 - val_loss: 0.5419 - val_acc: 0.7708\n",
      "Epoch 80/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3863 - acc: 0.8235 - val_loss: 0.6499 - val_acc: 0.7083\n",
      "Epoch 81/200\n",
      "Learning rate:  0.001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3797 - acc: 0.8295 - val_loss: 0.5470 - val_acc: 0.7706\n",
      "Epoch 82/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3736 - acc: 0.8312 - val_loss: 0.5587 - val_acc: 0.7703\n",
      "Epoch 83/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3686 - acc: 0.8339 - val_loss: 0.5732 - val_acc: 0.7680\n",
      "Epoch 84/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3694 - acc: 0.8315 - val_loss: 0.5730 - val_acc: 0.7684\n",
      "Epoch 85/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3687 - acc: 0.8280 - val_loss: 0.5773 - val_acc: 0.7680\n",
      "Epoch 86/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3697 - acc: 0.8315 - val_loss: 0.5954 - val_acc: 0.7575\n",
      "Epoch 87/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3665 - acc: 0.8330 - val_loss: 0.5854 - val_acc: 0.7656\n",
      "Epoch 88/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3679 - acc: 0.8325 - val_loss: 0.5712 - val_acc: 0.7706\n",
      "Epoch 89/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3667 - acc: 0.8321 - val_loss: 0.5911 - val_acc: 0.7612\n",
      "Epoch 90/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3663 - acc: 0.8356 - val_loss: 0.6071 - val_acc: 0.7460\n",
      "Epoch 91/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3706 - acc: 0.8306 - val_loss: 0.5821 - val_acc: 0.7673\n",
      "Epoch 92/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3686 - acc: 0.8316 - val_loss: 0.5889 - val_acc: 0.7668\n",
      "Epoch 93/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3684 - acc: 0.8322 - val_loss: 0.5864 - val_acc: 0.7699\n",
      "Epoch 94/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3696 - acc: 0.8300 - val_loss: 0.5878 - val_acc: 0.7670\n",
      "Epoch 95/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3672 - acc: 0.8330 - val_loss: 0.5764 - val_acc: 0.7711\n",
      "Epoch 96/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3661 - acc: 0.8314 - val_loss: 0.5759 - val_acc: 0.7717\n",
      "Epoch 97/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3648 - acc: 0.8351 - val_loss: 0.5828 - val_acc: 0.7701\n",
      "Epoch 98/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 124ms/step - loss: 0.3644 - acc: 0.8338 - val_loss: 0.5768 - val_acc: 0.7723\n",
      "Epoch 99/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3619 - acc: 0.8340 - val_loss: 0.5892 - val_acc: 0.7659\n",
      "Epoch 100/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3653 - acc: 0.8330 - val_loss: 0.5723 - val_acc: 0.7735\n",
      "Epoch 101/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3683 - acc: 0.8311 - val_loss: 0.5754 - val_acc: 0.7708\n",
      "Epoch 102/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3636 - acc: 0.8336 - val_loss: 0.5853 - val_acc: 0.7634\n",
      "Epoch 103/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 124ms/step - loss: 0.3626 - acc: 0.8335 - val_loss: 0.5891 - val_acc: 0.7652\n",
      "Epoch 104/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3637 - acc: 0.8348 - val_loss: 0.5902 - val_acc: 0.7630\n",
      "Epoch 105/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3658 - acc: 0.8321 - val_loss: 0.5884 - val_acc: 0.7658\n",
      "Epoch 106/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3645 - acc: 0.8357 - val_loss: 0.5795 - val_acc: 0.7704\n",
      "Epoch 107/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3644 - acc: 0.8330 - val_loss: 0.5742 - val_acc: 0.7705\n",
      "Epoch 108/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3655 - acc: 0.8370 - val_loss: 0.5877 - val_acc: 0.7659\n",
      "Epoch 109/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3655 - acc: 0.8338 - val_loss: 0.6032 - val_acc: 0.7536\n",
      "Epoch 110/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3646 - acc: 0.8340 - val_loss: 0.5739 - val_acc: 0.7684\n",
      "Epoch 111/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3645 - acc: 0.8338 - val_loss: 0.5844 - val_acc: 0.7676\n",
      "Epoch 112/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3644 - acc: 0.8364 - val_loss: 0.5876 - val_acc: 0.7628\n",
      "Epoch 113/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3621 - acc: 0.8351 - val_loss: 0.5874 - val_acc: 0.7658\n",
      "Epoch 114/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3668 - acc: 0.8338 - val_loss: 0.5824 - val_acc: 0.7721\n",
      "Epoch 115/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3629 - acc: 0.8360 - val_loss: 0.5809 - val_acc: 0.7707\n",
      "Epoch 116/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3629 - acc: 0.8344 - val_loss: 0.5606 - val_acc: 0.7713\n",
      "Epoch 117/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3595 - acc: 0.8353 - val_loss: 0.5658 - val_acc: 0.7723\n",
      "Epoch 118/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3619 - acc: 0.8334 - val_loss: 0.5768 - val_acc: 0.7681\n",
      "Epoch 119/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3611 - acc: 0.8368 - val_loss: 0.5719 - val_acc: 0.7713\n",
      "Epoch 120/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3631 - acc: 0.8351 - val_loss: 0.6056 - val_acc: 0.7513\n",
      "Epoch 121/200\n",
      "Learning rate:  0.0001\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3590 - acc: 0.8381 - val_loss: 0.5792 - val_acc: 0.7693\n",
      "Epoch 122/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3633 - acc: 0.8357 - val_loss: 0.5840 - val_acc: 0.7675\n",
      "Epoch 123/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3597 - acc: 0.8358 - val_loss: 0.5865 - val_acc: 0.7667\n",
      "Epoch 124/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3585 - acc: 0.8354 - val_loss: 0.5856 - val_acc: 0.7669\n",
      "Epoch 125/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 124ms/step - loss: 0.3616 - acc: 0.8344 - val_loss: 0.5873 - val_acc: 0.7663\n",
      "Epoch 126/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3629 - acc: 0.8337 - val_loss: 0.5881 - val_acc: 0.7662\n",
      "Epoch 127/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3584 - acc: 0.8374 - val_loss: 0.5916 - val_acc: 0.7645\n",
      "Epoch 128/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3640 - acc: 0.8336 - val_loss: 0.5918 - val_acc: 0.7647\n",
      "Epoch 129/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3619 - acc: 0.8346 - val_loss: 0.5936 - val_acc: 0.7635\n",
      "Epoch 130/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3589 - acc: 0.8367 - val_loss: 0.5921 - val_acc: 0.7648\n",
      "Epoch 131/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3631 - acc: 0.8341 - val_loss: 0.5936 - val_acc: 0.7644\n",
      "Epoch 132/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 124ms/step - loss: 0.3629 - acc: 0.8349 - val_loss: 0.5956 - val_acc: 0.7621\n",
      "Epoch 133/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3603 - acc: 0.8348 - val_loss: 0.5996 - val_acc: 0.7583\n",
      "Epoch 134/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3616 - acc: 0.8354 - val_loss: 0.5992 - val_acc: 0.7578\n",
      "Epoch 135/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3588 - acc: 0.8376 - val_loss: 0.5961 - val_acc: 0.7600\n",
      "Epoch 136/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3573 - acc: 0.8392 - val_loss: 0.5906 - val_acc: 0.7647\n",
      "Epoch 137/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3597 - acc: 0.8385 - val_loss: 0.5877 - val_acc: 0.7669\n",
      "Epoch 138/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3632 - acc: 0.8354 - val_loss: 0.5857 - val_acc: 0.7676\n",
      "Epoch 139/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3593 - acc: 0.8348 - val_loss: 0.5883 - val_acc: 0.7663\n",
      "Epoch 140/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3590 - acc: 0.8378 - val_loss: 0.5891 - val_acc: 0.7649\n",
      "Epoch 141/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3582 - acc: 0.8391 - val_loss: 0.5886 - val_acc: 0.7659\n",
      "Epoch 142/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3626 - acc: 0.8327 - val_loss: 0.5863 - val_acc: 0.7669\n",
      "Epoch 143/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3599 - acc: 0.8380 - val_loss: 0.5855 - val_acc: 0.7674\n",
      "Epoch 144/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3584 - acc: 0.8363 - val_loss: 0.5885 - val_acc: 0.7655\n",
      "Epoch 145/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3620 - acc: 0.8356 - val_loss: 0.5921 - val_acc: 0.7632\n",
      "Epoch 146/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3586 - acc: 0.8378 - val_loss: 0.5888 - val_acc: 0.7651\n",
      "Epoch 147/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3619 - acc: 0.8350 - val_loss: 0.5899 - val_acc: 0.7646\n",
      "Epoch 148/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3577 - acc: 0.8384 - val_loss: 0.5907 - val_acc: 0.7644\n",
      "Epoch 149/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3601 - acc: 0.8376 - val_loss: 0.5936 - val_acc: 0.7626\n",
      "Epoch 150/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3592 - acc: 0.8363 - val_loss: 0.5898 - val_acc: 0.7651\n",
      "Epoch 151/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3626 - acc: 0.8356 - val_loss: 0.5887 - val_acc: 0.7656\n",
      "Epoch 152/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3601 - acc: 0.8364 - val_loss: 0.5886 - val_acc: 0.7653\n",
      "Epoch 153/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3583 - acc: 0.8367 - val_loss: 0.5924 - val_acc: 0.7628\n",
      "Epoch 154/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3593 - acc: 0.8377 - val_loss: 0.5898 - val_acc: 0.7651\n",
      "Epoch 155/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3597 - acc: 0.8387 - val_loss: 0.5875 - val_acc: 0.7668\n",
      "Epoch 156/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 127ms/step - loss: 0.3593 - acc: 0.8372 - val_loss: 0.5877 - val_acc: 0.7663\n",
      "Epoch 157/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3598 - acc: 0.8341 - val_loss: 0.5857 - val_acc: 0.7669\n",
      "Epoch 158/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3585 - acc: 0.8342 - val_loss: 0.5832 - val_acc: 0.7680\n",
      "Epoch 159/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3604 - acc: 0.8353 - val_loss: 0.5887 - val_acc: 0.7654\n",
      "Epoch 160/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3609 - acc: 0.8338 - val_loss: 0.5880 - val_acc: 0.7662\n",
      "Epoch 161/200\n",
      "Learning rate:  1e-05\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3608 - acc: 0.8323 - val_loss: 0.5872 - val_acc: 0.7665\n",
      "Epoch 162/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3587 - acc: 0.8358 - val_loss: 0.5870 - val_acc: 0.7669\n",
      "Epoch 163/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3595 - acc: 0.8352 - val_loss: 0.5869 - val_acc: 0.7667\n",
      "Epoch 164/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3602 - acc: 0.8346 - val_loss: 0.5868 - val_acc: 0.7668\n",
      "Epoch 165/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3604 - acc: 0.8361 - val_loss: 0.5865 - val_acc: 0.7669\n",
      "Epoch 166/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3607 - acc: 0.8348 - val_loss: 0.5868 - val_acc: 0.7672\n",
      "Epoch 167/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 127ms/step - loss: 0.3600 - acc: 0.8372 - val_loss: 0.5868 - val_acc: 0.7670\n",
      "Epoch 168/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3593 - acc: 0.8358 - val_loss: 0.5871 - val_acc: 0.7668\n",
      "Epoch 169/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3590 - acc: 0.8367 - val_loss: 0.5872 - val_acc: 0.7667\n",
      "Epoch 170/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3593 - acc: 0.8369 - val_loss: 0.5871 - val_acc: 0.7668\n",
      "Epoch 171/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3572 - acc: 0.8379 - val_loss: 0.5878 - val_acc: 0.7663\n",
      "Epoch 172/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3633 - acc: 0.8356 - val_loss: 0.5869 - val_acc: 0.7669\n",
      "Epoch 173/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 127ms/step - loss: 0.3607 - acc: 0.8368 - val_loss: 0.5876 - val_acc: 0.7665\n",
      "Epoch 174/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3565 - acc: 0.8408 - val_loss: 0.5872 - val_acc: 0.7665\n",
      "Epoch 175/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3617 - acc: 0.8364 - val_loss: 0.5866 - val_acc: 0.7669\n",
      "Epoch 176/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3601 - acc: 0.8366 - val_loss: 0.5869 - val_acc: 0.7669\n",
      "Epoch 177/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3599 - acc: 0.8368 - val_loss: 0.5880 - val_acc: 0.7663\n",
      "Epoch 178/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3610 - acc: 0.8346 - val_loss: 0.5874 - val_acc: 0.7664\n",
      "Epoch 179/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3586 - acc: 0.8380 - val_loss: 0.5880 - val_acc: 0.7662\n",
      "Epoch 180/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3572 - acc: 0.8371 - val_loss: 0.5881 - val_acc: 0.7659\n",
      "Epoch 181/200\n",
      "Learning rate:  1e-06\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3590 - acc: 0.8362 - val_loss: 0.5878 - val_acc: 0.7661\n",
      "Epoch 182/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3585 - acc: 0.8368 - val_loss: 0.5873 - val_acc: 0.7666\n",
      "Epoch 183/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3609 - acc: 0.8369 - val_loss: 0.5874 - val_acc: 0.7665\n",
      "Epoch 184/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3566 - acc: 0.8385 - val_loss: 0.5869 - val_acc: 0.7667\n",
      "Epoch 185/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3607 - acc: 0.8362 - val_loss: 0.5875 - val_acc: 0.7665\n",
      "Epoch 186/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3566 - acc: 0.8395 - val_loss: 0.5870 - val_acc: 0.7663\n",
      "Epoch 187/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3592 - acc: 0.8383 - val_loss: 0.5872 - val_acc: 0.7667\n",
      "Epoch 188/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3583 - acc: 0.8340 - val_loss: 0.5868 - val_acc: 0.7669\n",
      "Epoch 189/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3593 - acc: 0.8386 - val_loss: 0.5869 - val_acc: 0.7663\n",
      "Epoch 190/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3588 - acc: 0.8360 - val_loss: 0.5873 - val_acc: 0.7663\n",
      "Epoch 191/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3580 - acc: 0.8347 - val_loss: 0.5866 - val_acc: 0.7670\n",
      "Epoch 192/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3610 - acc: 0.8392 - val_loss: 0.5869 - val_acc: 0.7663\n",
      "Epoch 193/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3606 - acc: 0.8368 - val_loss: 0.5861 - val_acc: 0.7670\n",
      "Epoch 194/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 127ms/step - loss: 0.3593 - acc: 0.8375 - val_loss: 0.5865 - val_acc: 0.7670\n",
      "Epoch 195/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3622 - acc: 0.8372 - val_loss: 0.5865 - val_acc: 0.7671\n",
      "Epoch 196/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3596 - acc: 0.8370 - val_loss: 0.5867 - val_acc: 0.7667\n",
      "Epoch 197/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 125ms/step - loss: 0.3583 - acc: 0.8367 - val_loss: 0.5871 - val_acc: 0.7665\n",
      "Epoch 198/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3602 - acc: 0.8369 - val_loss: 0.5858 - val_acc: 0.7671\n",
      "Epoch 199/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3603 - acc: 0.8353 - val_loss: 0.5864 - val_acc: 0.7669\n",
      "Epoch 200/200\n",
      "Learning rate:  5e-07\n",
      "118/117 [==============================] - 15s 126ms/step - loss: 0.3598 - acc: 0.8367 - val_loss: 0.5871 - val_acc: 0.7669\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "cnn_classifier_train = cnn_classifier.fit_generator(datagen.flow(x_train, labels_train, batch_size = 128),\n",
    "                epochs=epochs,steps_per_epoch=len(x_train) / 128,\n",
    "                shuffle=True,verbose = 1,\n",
    "                validation_data=(x_validation, labels_test), callbacks = callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "kyidIZgtTpqa",
    "outputId": "34b2e1be-43cc-4c23-b444-d71525f3ef04"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJu6GYRgNkP8HTPgz+gD9OBIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = cnn_classifier_train.history['acc']\n",
    "val_acc = cnn_classifier_train.history['val_acc']\n",
    "epochs = range(200)\n",
    "plt.figure()\n",
    "plt.plot(epochs, acc, 'go --', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aroGL4c-CL9M"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "cnn_classifier.save('/content/gdrive/My Drive/Chexnet/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XvlcxjHaEZKc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "Y_prediction = cnn_classifier.predict(x_validation)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(labels_test,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "FJ-ZqMHGFf52",
    "outputId": "e6404abf-843f-43ea-dfa0-4599c659e99e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAHVCAYAAAA5NRumAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt=\"d\");"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ChexNET_CNNv2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
