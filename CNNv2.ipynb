{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PvWONVSVTpp6",
    "outputId": "3954891d-af47-4fb2-e053-d9445d64fa13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "l7kj2Wh3Tpp_",
    "outputId": "d55f9e75-c57b-4873-e012-2e877156cc10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# The Model\n",
    "input_img = Input(shape=(64, 64, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "# Conv Layer 1\n",
    "x = Conv2D(64, (3,3), padding = 'same')(input_img)\n",
    "x = BatchNormalization(axis = 3)(x)\n",
    "x = Activation('relu')(x)\n",
    "# Conv Layer 2\n",
    "x = Conv2D(32, (3,3), padding = 'same')(x)\n",
    "x = Conv2D(32, (3,3), padding = 'same')(x)\n",
    "x = Conv2D(32, (3,3), padding = 'same')(x)\n",
    "x = BatchNormalization(axis = 3)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size = (2,2)) (x)\n",
    "# Conv Layer 1\n",
    "x = Conv2D(64, (3,3), padding = 'same')(x)\n",
    "x = Conv2D(64, (3,3), padding = 'same')(x)\n",
    "x = Conv2D(64, (3,3), padding = 'same')(x)\n",
    "x = BatchNormalization(axis = 3)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size = (2,2)) (x)\n",
    "\n",
    "# Conv Layer 1\n",
    "x = Conv2D(32, (3,3), padding = 'same')(x)\n",
    "x = Conv2D(32, (3,3), padding = 'same')(x)\n",
    "x = Conv2D(32, (3,3), padding = 'same')(x)\n",
    "x = BatchNormalization(axis = 3)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size = (2,2)) (x)\n",
    "\n",
    "\n",
    "# Dense\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation = 'relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(512, activation = 'relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(512, activation = 'relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "outputs = Dense(2, activation='softmax')(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cnn_classifier = Model(input_img, outputs)\n",
    "#adam = optimizers.Adam(lr=0.0003, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#sgd = optimizers.SGD(lr=0.001, decay=1e-4, momentum=0.8, nesterov=True)\n",
    "cnn_classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1200
    },
    "colab_type": "code",
    "id": "X9Hn5dslTpqC",
    "outputId": "975860ac-33c5-4900-efad-560e8d0bd11a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 3,185,154\n",
      "Trainable params: 3,184,770\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "6DpZ0anwUUhI",
    "outputId": "6793c880-2890-423d-cdb2-551005f3db6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Cqmq876SUolk",
    "outputId": "5bb602a8-ac16-4a68-9e67-0a831fbec622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_data.npy\ttrain_v2.csv\t Untitled.ipynb\n",
      "my_model.h5\tUntitled0.ipynb  VAE_CHEXNET.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls \"/content/gdrive/My Drive/Chexnet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z9mkJHh8D5zP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "18-AzRpFTpqF",
    "outputId": "9b18dded-7438-4dec-b561-87401da9d528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26684, 4096)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/content/gdrive/My Drive/Chexnet/image_data.npy')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qpsg4U9FTpqI"
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('train_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGVq42GyTpqK"
   },
   "outputs": [],
   "source": [
    "x_train = data[:20000,:]\n",
    "x_validation = data[20000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iobOfykTTpqM",
    "outputId": "81a2505a-a1c0-4567-8889-97ea2c82f991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 4096)\n",
      "(6684, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hULoD0qgTpqP"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_validation = x_validation.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "BAC6pv829RMM",
    "outputId": "87976d59-72a4-47f0-a0e4-426656f8a1d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1)\n",
      "(array([0., 1.]), array([15154,  4846]))\n",
      "(array([0., 1.]), array([7154, 4846]))\n",
      "(12000, 4097)\n",
      "(6684, 1)\n",
      "(array([0., 1.]), array([5518, 1166]))\n",
      "(array([0., 1.]), array([1518, 1166]))\n",
      "(2684, 4097)\n"
     ]
    }
   ],
   "source": [
    "import keras.utils\n",
    "df = pd.read_csv('/content/gdrive/My Drive/Chexnet/train_v2.csv')\n",
    "labels_train = df['Target'][:20000]\n",
    "labels_test = df['Target'][20000:]\n",
    "labels_train = np.asarray(labels_train).reshape(20000,-1)\n",
    "print(np.shape(labels_train))\n",
    "dataset = np.hstack((x_train,labels_train))\n",
    "print(np.unique(dataset[:,-1], return_counts = True))\n",
    "dataset = dataset[dataset[:,-1].argsort()]\n",
    "dataset = dataset[8000:,:]\n",
    "print(np.unique(dataset[:,-1], return_counts = True))\n",
    "np.random.shuffle(dataset)\n",
    "print(np.shape(dataset))\n",
    "x_train = dataset[:,:4096]\n",
    "labels_train = dataset[:,-1]\n",
    "\n",
    "labels_test = np.asarray(labels_test).reshape(6684,-1)\n",
    "print(np.shape(labels_test))\n",
    "dataset2 = np.hstack((x_validation,labels_test))\n",
    "print(np.unique(dataset2[:,-1], return_counts = True))\n",
    "dataset2 = dataset2[dataset2[:,-1].argsort()]\n",
    "dataset2 = dataset2[4000:,:]\n",
    "print(np.unique(dataset2[:,-1], return_counts = True))\n",
    "np.random.shuffle(dataset2)\n",
    "print(np.shape(dataset2))\n",
    "x_validation = dataset2[:,:4096]\n",
    "labels_test = dataset2[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EP8SJv3GBNYi",
    "outputId": "2118b62c-3189-4ccf-f4f9-9562747a4af7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 4096)"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FwRe8CApA650"
   },
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (len(x_train), 64, 64, 1))  # adapt this if using `channels_first` image data format\n",
    "x_validation = np.reshape(x_validation, (len(x_validation), 64, 64, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m596xQiroXey"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# Data Generator Module\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NxuTnb-3f5TD"
   },
   "outputs": [],
   "source": [
    "labels_train = keras.utils.to_categorical(labels_train)\n",
    "labels_test = keras.utils.to_categorical(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYOTWd-SjbCS"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "lr_reducer = ReduceLROnPlateau(monitor='loss',factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "callbacks = [lr_scheduler, lr_reducer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10374
    },
    "colab_type": "code",
    "id": "r69-npu7TpqW",
    "outputId": "9095035e-8955-4c94-f496-1903158078c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 15s 160ms/step - loss: 0.7289 - acc: 0.5738 - val_loss: 0.7923 - val_acc: 0.5656\n",
      "Epoch 2/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 104ms/step - loss: 0.6085 - acc: 0.6765 - val_loss: 1.1475 - val_acc: 0.4344\n",
      "Epoch 3/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 104ms/step - loss: 0.5738 - acc: 0.7125 - val_loss: 0.8425 - val_acc: 0.4344\n",
      "Epoch 4/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 105ms/step - loss: 0.5629 - acc: 0.7243 - val_loss: 1.0726 - val_acc: 0.5659\n",
      "Epoch 5/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 106ms/step - loss: 0.5486 - acc: 0.7367 - val_loss: 1.6087 - val_acc: 0.4344\n",
      "Epoch 6/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 107ms/step - loss: 0.5476 - acc: 0.7343 - val_loss: 0.6409 - val_acc: 0.6554\n",
      "Epoch 7/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 107ms/step - loss: 0.5451 - acc: 0.7374 - val_loss: 0.6683 - val_acc: 0.6080\n",
      "Epoch 8/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 107ms/step - loss: 0.5394 - acc: 0.7394 - val_loss: 0.6360 - val_acc: 0.6818\n",
      "Epoch 9/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 107ms/step - loss: 0.5380 - acc: 0.7350 - val_loss: 1.1057 - val_acc: 0.4344\n",
      "Epoch 10/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 108ms/step - loss: 0.5313 - acc: 0.7395 - val_loss: 0.6478 - val_acc: 0.6121\n",
      "Epoch 11/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 108ms/step - loss: 0.5343 - acc: 0.7371 - val_loss: 0.6482 - val_acc: 0.6475\n",
      "Epoch 12/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 108ms/step - loss: 0.5312 - acc: 0.7449 - val_loss: 0.7673 - val_acc: 0.4341\n",
      "Epoch 13/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 108ms/step - loss: 0.5259 - acc: 0.7448 - val_loss: 0.7295 - val_acc: 0.4344\n",
      "Epoch 14/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 108ms/step - loss: 0.5262 - acc: 0.7475 - val_loss: 0.7224 - val_acc: 0.4322\n",
      "Epoch 15/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 108ms/step - loss: 0.5260 - acc: 0.7438 - val_loss: 1.2876 - val_acc: 0.4348\n",
      "Epoch 16/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 109ms/step - loss: 0.5213 - acc: 0.7501 - val_loss: 0.7688 - val_acc: 0.4355\n",
      "Epoch 17/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 109ms/step - loss: 0.5216 - acc: 0.7487 - val_loss: 0.7010 - val_acc: 0.4508\n",
      "Epoch 18/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 109ms/step - loss: 0.5204 - acc: 0.7503 - val_loss: 0.7519 - val_acc: 0.4352\n",
      "Epoch 19/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 109ms/step - loss: 0.5208 - acc: 0.7493 - val_loss: 0.8611 - val_acc: 0.4322\n",
      "Epoch 20/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 110ms/step - loss: 0.5237 - acc: 0.7481 - val_loss: 0.8684 - val_acc: 0.4359\n",
      "Epoch 21/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5203 - acc: 0.7458 - val_loss: 0.8509 - val_acc: 0.5656\n",
      "Epoch 22/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 110ms/step - loss: 0.5170 - acc: 0.7494 - val_loss: 0.6144 - val_acc: 0.6773\n",
      "Epoch 23/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5171 - acc: 0.7450 - val_loss: 0.7068 - val_acc: 0.5693\n",
      "Epoch 24/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5152 - acc: 0.7511 - val_loss: 0.7137 - val_acc: 0.5041\n",
      "Epoch 25/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5107 - acc: 0.7518 - val_loss: 0.6815 - val_acc: 0.5257\n",
      "Epoch 26/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5137 - acc: 0.7516 - val_loss: 0.7087 - val_acc: 0.4840\n",
      "Epoch 27/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5133 - acc: 0.7536 - val_loss: 0.6222 - val_acc: 0.6997\n",
      "Epoch 28/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5097 - acc: 0.7544 - val_loss: 0.7379 - val_acc: 0.4426\n",
      "Epoch 29/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5113 - acc: 0.7526 - val_loss: 0.7164 - val_acc: 0.4426\n",
      "Epoch 30/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5086 - acc: 0.7540 - val_loss: 0.7245 - val_acc: 0.5663\n",
      "Epoch 31/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5077 - acc: 0.7544 - val_loss: 0.7030 - val_acc: 0.4717\n",
      "Epoch 32/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5089 - acc: 0.7540 - val_loss: 0.6922 - val_acc: 0.5026\n",
      "Epoch 33/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5050 - acc: 0.7551 - val_loss: 0.9312 - val_acc: 0.4344\n",
      "Epoch 34/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5070 - acc: 0.7576 - val_loss: 0.6761 - val_acc: 0.5645\n",
      "Epoch 35/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5022 - acc: 0.7589 - val_loss: 0.6838 - val_acc: 0.5000\n",
      "Epoch 36/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5015 - acc: 0.7570 - val_loss: 0.7300 - val_acc: 0.4426\n",
      "Epoch 37/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 112ms/step - loss: 0.5056 - acc: 0.7541 - val_loss: 0.6252 - val_acc: 0.7004\n",
      "Epoch 38/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5009 - acc: 0.7587 - val_loss: 0.7197 - val_acc: 0.4359\n",
      "Epoch 39/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5024 - acc: 0.7547 - val_loss: 0.6125 - val_acc: 0.6248\n",
      "Epoch 40/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5024 - acc: 0.7543 - val_loss: 0.7883 - val_acc: 0.4359\n",
      "Epoch 41/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4980 - acc: 0.7614 - val_loss: 1.1455 - val_acc: 0.4344\n",
      "Epoch 42/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4999 - acc: 0.7602 - val_loss: 1.6831 - val_acc: 0.4344\n",
      "Epoch 43/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5007 - acc: 0.7591 - val_loss: 0.6552 - val_acc: 0.5659\n",
      "Epoch 44/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4969 - acc: 0.7617 - val_loss: 0.6348 - val_acc: 0.6624\n",
      "Epoch 45/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4994 - acc: 0.7575 - val_loss: 0.7791 - val_acc: 0.4352\n",
      "Epoch 46/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4956 - acc: 0.7612 - val_loss: 0.6603 - val_acc: 0.6360\n",
      "Epoch 47/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5005 - acc: 0.7583 - val_loss: 0.7267 - val_acc: 0.4609\n",
      "Epoch 48/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4941 - acc: 0.7630 - val_loss: 0.6986 - val_acc: 0.4709\n",
      "Epoch 49/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4970 - acc: 0.7610 - val_loss: 0.7128 - val_acc: 0.5738\n",
      "Epoch 50/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4944 - acc: 0.7610 - val_loss: 0.7488 - val_acc: 0.4355\n",
      "Epoch 51/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4956 - acc: 0.7579 - val_loss: 0.6554 - val_acc: 0.6785\n",
      "Epoch 52/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 11s 112ms/step - loss: 0.4941 - acc: 0.7629 - val_loss: 0.7294 - val_acc: 0.5756\n",
      "Epoch 53/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.5010 - acc: 0.7561 - val_loss: 0.6704 - val_acc: 0.6155\n",
      "Epoch 54/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4935 - acc: 0.7590 - val_loss: 0.6870 - val_acc: 0.4881\n",
      "Epoch 55/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4904 - acc: 0.7633 - val_loss: 0.6663 - val_acc: 0.5842\n",
      "Epoch 56/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4963 - acc: 0.7588 - val_loss: 0.7763 - val_acc: 0.4352\n",
      "Epoch 57/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4904 - acc: 0.7631 - val_loss: 0.6662 - val_acc: 0.6237\n",
      "Epoch 58/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4938 - acc: 0.7566 - val_loss: 0.6966 - val_acc: 0.5123\n",
      "Epoch 59/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4954 - acc: 0.7620 - val_loss: 0.8060 - val_acc: 0.4363\n",
      "Epoch 60/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4964 - acc: 0.7623 - val_loss: 0.7327 - val_acc: 0.4512\n",
      "Epoch 61/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4928 - acc: 0.7593 - val_loss: 0.6496 - val_acc: 0.6274\n",
      "Epoch 62/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4943 - acc: 0.7631 - val_loss: 0.6639 - val_acc: 0.5697\n",
      "Epoch 63/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4911 - acc: 0.7619 - val_loss: 0.7122 - val_acc: 0.4426\n",
      "Epoch 64/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4936 - acc: 0.7626 - val_loss: 0.6553 - val_acc: 0.6580\n",
      "Epoch 65/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4912 - acc: 0.7628 - val_loss: 0.6891 - val_acc: 0.5496\n",
      "Epoch 66/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 11s 112ms/step - loss: 0.4888 - acc: 0.7642 - val_loss: 0.6547 - val_acc: 0.5999\n",
      "Epoch 67/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 112ms/step - loss: 0.4897 - acc: 0.7600 - val_loss: 0.6453 - val_acc: 0.6028\n",
      "Epoch 68/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4900 - acc: 0.7632 - val_loss: 0.9167 - val_acc: 0.4355\n",
      "Epoch 69/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4856 - acc: 0.7664 - val_loss: 0.7215 - val_acc: 0.4907\n",
      "Epoch 70/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4869 - acc: 0.7644 - val_loss: 0.6829 - val_acc: 0.5678\n",
      "Epoch 71/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4885 - acc: 0.7665 - val_loss: 0.6620 - val_acc: 0.6114\n",
      "Epoch 72/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4856 - acc: 0.7644 - val_loss: 0.7018 - val_acc: 0.4676\n",
      "Epoch 73/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4906 - acc: 0.7629 - val_loss: 0.8072 - val_acc: 0.4363\n",
      "Epoch 74/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4870 - acc: 0.7629 - val_loss: 0.7873 - val_acc: 0.4359\n",
      "Epoch 75/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4882 - acc: 0.7634 - val_loss: 0.6761 - val_acc: 0.6051\n",
      "Epoch 76/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4861 - acc: 0.7653 - val_loss: 0.6868 - val_acc: 0.4903\n",
      "Epoch 77/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4864 - acc: 0.7657 - val_loss: 0.6387 - val_acc: 0.6852\n",
      "Epoch 78/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4873 - acc: 0.7665 - val_loss: 1.0306 - val_acc: 0.4344\n",
      "Epoch 79/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4897 - acc: 0.7630 - val_loss: 0.6905 - val_acc: 0.4970\n",
      "Epoch 80/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4828 - acc: 0.7658 - val_loss: 0.7015 - val_acc: 0.5011\n",
      "Epoch 81/200\n",
      "Learning rate:  0.001\n",
      "94/93 [==============================] - 10s 112ms/step - loss: 0.4872 - acc: 0.7662 - val_loss: 0.7800 - val_acc: 0.4348\n",
      "Epoch 82/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4790 - acc: 0.7665 - val_loss: 0.6449 - val_acc: 0.6632\n",
      "Epoch 83/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4723 - acc: 0.7699 - val_loss: 0.6374 - val_acc: 0.6896\n",
      "Epoch 84/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4709 - acc: 0.7727 - val_loss: 0.6647 - val_acc: 0.5920\n",
      "Epoch 85/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4693 - acc: 0.7760 - val_loss: 0.6703 - val_acc: 0.5760\n",
      "Epoch 86/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4739 - acc: 0.7691 - val_loss: 0.6635 - val_acc: 0.5950\n",
      "Epoch 87/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4724 - acc: 0.7735 - val_loss: 0.6579 - val_acc: 0.6051\n",
      "Epoch 88/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4680 - acc: 0.7763 - val_loss: 0.6806 - val_acc: 0.5410\n",
      "Epoch 89/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4683 - acc: 0.7739 - val_loss: 0.6614 - val_acc: 0.5924\n",
      "Epoch 90/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4687 - acc: 0.7739 - val_loss: 0.6685 - val_acc: 0.5618\n",
      "Epoch 91/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4657 - acc: 0.7731 - val_loss: 0.6821 - val_acc: 0.5134\n",
      "Epoch 92/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4708 - acc: 0.7727 - val_loss: 0.6645 - val_acc: 0.5857\n",
      "Epoch 93/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4692 - acc: 0.7737 - val_loss: 0.6732 - val_acc: 0.5473\n",
      "Epoch 94/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4670 - acc: 0.7726 - val_loss: 0.6529 - val_acc: 0.6352\n",
      "Epoch 95/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4631 - acc: 0.7768 - val_loss: 0.6726 - val_acc: 0.5574\n",
      "Epoch 96/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 11s 112ms/step - loss: 0.4674 - acc: 0.7749 - val_loss: 0.6588 - val_acc: 0.6103\n",
      "Epoch 97/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4651 - acc: 0.7763 - val_loss: 0.6897 - val_acc: 0.4989\n",
      "Epoch 98/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4690 - acc: 0.7702 - val_loss: 0.6691 - val_acc: 0.5913\n",
      "Epoch 99/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4659 - acc: 0.7773 - val_loss: 0.6677 - val_acc: 0.5876\n",
      "Epoch 100/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4691 - acc: 0.7720 - val_loss: 0.6433 - val_acc: 0.6721\n",
      "Epoch 101/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4654 - acc: 0.7774 - val_loss: 0.6579 - val_acc: 0.6390\n",
      "Epoch 102/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4638 - acc: 0.7764 - val_loss: 0.6533 - val_acc: 0.6468\n",
      "Epoch 103/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4644 - acc: 0.7726 - val_loss: 0.6539 - val_acc: 0.6487\n",
      "Epoch 104/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4647 - acc: 0.7721 - val_loss: 0.6677 - val_acc: 0.5924\n",
      "Epoch 105/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4661 - acc: 0.7731 - val_loss: 0.6808 - val_acc: 0.5518\n",
      "Epoch 106/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4641 - acc: 0.7756 - val_loss: 0.6428 - val_acc: 0.6863\n",
      "Epoch 107/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4643 - acc: 0.7759 - val_loss: 0.6800 - val_acc: 0.5496\n",
      "Epoch 108/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4644 - acc: 0.7758 - val_loss: 0.6674 - val_acc: 0.5961\n",
      "Epoch 109/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4622 - acc: 0.7763 - val_loss: 0.6692 - val_acc: 0.6051\n",
      "Epoch 110/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4627 - acc: 0.7781 - val_loss: 0.6549 - val_acc: 0.6606\n",
      "Epoch 111/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4611 - acc: 0.7741 - val_loss: 0.6383 - val_acc: 0.6863\n",
      "Epoch 112/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4608 - acc: 0.7789 - val_loss: 0.6536 - val_acc: 0.6580\n",
      "Epoch 113/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4607 - acc: 0.7789 - val_loss: 0.6640 - val_acc: 0.6118\n",
      "Epoch 114/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 112ms/step - loss: 0.4623 - acc: 0.7791 - val_loss: 0.6872 - val_acc: 0.4959\n",
      "Epoch 115/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4604 - acc: 0.7756 - val_loss: 0.6712 - val_acc: 0.5730\n",
      "Epoch 116/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4620 - acc: 0.7755 - val_loss: 0.6622 - val_acc: 0.6289\n",
      "Epoch 117/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4627 - acc: 0.7786 - val_loss: 0.6474 - val_acc: 0.6710\n",
      "Epoch 118/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4616 - acc: 0.7766 - val_loss: 0.6445 - val_acc: 0.6576\n",
      "Epoch 119/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4618 - acc: 0.7760 - val_loss: 0.6412 - val_acc: 0.6855\n",
      "Epoch 120/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4623 - acc: 0.7742 - val_loss: 0.6722 - val_acc: 0.5577\n",
      "Epoch 121/200\n",
      "Learning rate:  0.0001\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4596 - acc: 0.7786 - val_loss: 0.6532 - val_acc: 0.6639\n",
      "Epoch 122/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4632 - acc: 0.7793 - val_loss: 0.6653 - val_acc: 0.6006\n",
      "Epoch 123/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4599 - acc: 0.7773 - val_loss: 0.6677 - val_acc: 0.5861\n",
      "Epoch 124/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4631 - acc: 0.7728 - val_loss: 0.6665 - val_acc: 0.5909\n",
      "Epoch 125/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4598 - acc: 0.7776 - val_loss: 0.6641 - val_acc: 0.6006\n",
      "Epoch 126/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 11s 112ms/step - loss: 0.4625 - acc: 0.7785 - val_loss: 0.6670 - val_acc: 0.5872\n",
      "Epoch 127/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4602 - acc: 0.7762 - val_loss: 0.6633 - val_acc: 0.6032\n",
      "Epoch 128/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4595 - acc: 0.7757 - val_loss: 0.6602 - val_acc: 0.6181\n",
      "Epoch 129/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4606 - acc: 0.7756 - val_loss: 0.6614 - val_acc: 0.6103\n",
      "Epoch 130/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4588 - acc: 0.7772 - val_loss: 0.6629 - val_acc: 0.5999\n",
      "Epoch 131/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4600 - acc: 0.7776 - val_loss: 0.6635 - val_acc: 0.5984\n",
      "Epoch 132/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4597 - acc: 0.7783 - val_loss: 0.6622 - val_acc: 0.6110\n",
      "Epoch 133/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4610 - acc: 0.7787 - val_loss: 0.6660 - val_acc: 0.5898\n",
      "Epoch 134/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4579 - acc: 0.7802 - val_loss: 0.6632 - val_acc: 0.6021\n",
      "Epoch 135/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4595 - acc: 0.7762 - val_loss: 0.6644 - val_acc: 0.6017\n",
      "Epoch 136/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4613 - acc: 0.7780 - val_loss: 0.6649 - val_acc: 0.5972\n",
      "Epoch 137/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4582 - acc: 0.7785 - val_loss: 0.6649 - val_acc: 0.5972\n",
      "Epoch 138/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4591 - acc: 0.7756 - val_loss: 0.6620 - val_acc: 0.6129\n",
      "Epoch 139/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4617 - acc: 0.7780 - val_loss: 0.6612 - val_acc: 0.6185\n",
      "Epoch 140/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4572 - acc: 0.7784 - val_loss: 0.6595 - val_acc: 0.6285\n",
      "Epoch 141/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4600 - acc: 0.7753 - val_loss: 0.6568 - val_acc: 0.6401\n",
      "Epoch 142/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4573 - acc: 0.7807 - val_loss: 0.6576 - val_acc: 0.6349\n",
      "Epoch 143/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4586 - acc: 0.7772 - val_loss: 0.6578 - val_acc: 0.6356\n",
      "Epoch 144/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 112ms/step - loss: 0.4588 - acc: 0.7813 - val_loss: 0.6598 - val_acc: 0.6278\n",
      "Epoch 145/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4587 - acc: 0.7784 - val_loss: 0.6608 - val_acc: 0.6170\n",
      "Epoch 146/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4605 - acc: 0.7775 - val_loss: 0.6573 - val_acc: 0.6360\n",
      "Epoch 147/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4600 - acc: 0.7797 - val_loss: 0.6563 - val_acc: 0.6397\n",
      "Epoch 148/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4590 - acc: 0.7785 - val_loss: 0.6548 - val_acc: 0.6490\n",
      "Epoch 149/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4582 - acc: 0.7801 - val_loss: 0.6493 - val_acc: 0.6729\n",
      "Epoch 150/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4570 - acc: 0.7803 - val_loss: 0.6496 - val_acc: 0.6688\n",
      "Epoch 151/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4585 - acc: 0.7793 - val_loss: 0.6516 - val_acc: 0.6617\n",
      "Epoch 152/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4580 - acc: 0.7781 - val_loss: 0.6512 - val_acc: 0.6602\n",
      "Epoch 153/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4586 - acc: 0.7813 - val_loss: 0.6508 - val_acc: 0.6651\n",
      "Epoch 154/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4596 - acc: 0.7795 - val_loss: 0.6476 - val_acc: 0.6740\n",
      "Epoch 155/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4604 - acc: 0.7779 - val_loss: 0.6479 - val_acc: 0.6759\n",
      "Epoch 156/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4622 - acc: 0.7764 - val_loss: 0.6501 - val_acc: 0.6703\n",
      "Epoch 157/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4595 - acc: 0.7757 - val_loss: 0.6505 - val_acc: 0.6680\n",
      "Epoch 158/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4589 - acc: 0.7755 - val_loss: 0.6493 - val_acc: 0.6706\n",
      "Epoch 159/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4577 - acc: 0.7808 - val_loss: 0.6455 - val_acc: 0.6803\n",
      "Epoch 160/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4567 - acc: 0.7822 - val_loss: 0.6445 - val_acc: 0.6788\n",
      "Epoch 161/200\n",
      "Learning rate:  1e-05\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4583 - acc: 0.7767 - val_loss: 0.6467 - val_acc: 0.6781\n",
      "Epoch 162/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4573 - acc: 0.7821 - val_loss: 0.6485 - val_acc: 0.6736\n",
      "Epoch 163/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4606 - acc: 0.7788 - val_loss: 0.6494 - val_acc: 0.6718\n",
      "Epoch 164/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4574 - acc: 0.7798 - val_loss: 0.6498 - val_acc: 0.6714\n",
      "Epoch 165/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4587 - acc: 0.7789 - val_loss: 0.6499 - val_acc: 0.6692\n",
      "Epoch 166/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4585 - acc: 0.7820 - val_loss: 0.6495 - val_acc: 0.6721\n",
      "Epoch 167/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4574 - acc: 0.7810 - val_loss: 0.6497 - val_acc: 0.6699\n",
      "Epoch 168/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4582 - acc: 0.7794 - val_loss: 0.6503 - val_acc: 0.6692\n",
      "Epoch 169/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4589 - acc: 0.7814 - val_loss: 0.6494 - val_acc: 0.6695\n",
      "Epoch 170/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4620 - acc: 0.7751 - val_loss: 0.6486 - val_acc: 0.6710\n",
      "Epoch 171/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4624 - acc: 0.7790 - val_loss: 0.6491 - val_acc: 0.6703\n",
      "Epoch 172/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4593 - acc: 0.7816 - val_loss: 0.6488 - val_acc: 0.6703\n",
      "Epoch 173/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4587 - acc: 0.7775 - val_loss: 0.6487 - val_acc: 0.6706\n",
      "Epoch 174/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4571 - acc: 0.7781 - val_loss: 0.6495 - val_acc: 0.6688\n",
      "Epoch 175/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4597 - acc: 0.7801 - val_loss: 0.6500 - val_acc: 0.6673\n",
      "Epoch 176/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4574 - acc: 0.7785 - val_loss: 0.6504 - val_acc: 0.6677\n",
      "Epoch 177/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4600 - acc: 0.7779 - val_loss: 0.6504 - val_acc: 0.6673\n",
      "Epoch 178/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4564 - acc: 0.7765 - val_loss: 0.6500 - val_acc: 0.6677\n",
      "Epoch 179/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4592 - acc: 0.7760 - val_loss: 0.6502 - val_acc: 0.6662\n",
      "Epoch 180/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4599 - acc: 0.7803 - val_loss: 0.6501 - val_acc: 0.6673\n",
      "Epoch 181/200\n",
      "Learning rate:  1e-06\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4612 - acc: 0.7776 - val_loss: 0.6505 - val_acc: 0.6665\n",
      "Epoch 182/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4574 - acc: 0.7774 - val_loss: 0.6502 - val_acc: 0.6669\n",
      "Epoch 183/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4599 - acc: 0.7798 - val_loss: 0.6501 - val_acc: 0.6665\n",
      "Epoch 184/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4624 - acc: 0.7755 - val_loss: 0.6495 - val_acc: 0.6684\n",
      "Epoch 185/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 11s 112ms/step - loss: 0.4587 - acc: 0.7778 - val_loss: 0.6501 - val_acc: 0.6677\n",
      "Epoch 186/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4595 - acc: 0.7797 - val_loss: 0.6498 - val_acc: 0.6677\n",
      "Epoch 187/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4579 - acc: 0.7770 - val_loss: 0.6498 - val_acc: 0.6688\n",
      "Epoch 188/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4577 - acc: 0.7796 - val_loss: 0.6500 - val_acc: 0.6669\n",
      "Epoch 189/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4579 - acc: 0.7776 - val_loss: 0.6501 - val_acc: 0.6673\n",
      "Epoch 190/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4576 - acc: 0.7786 - val_loss: 0.6501 - val_acc: 0.6688\n",
      "Epoch 191/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4585 - acc: 0.7783 - val_loss: 0.6499 - val_acc: 0.6673\n",
      "Epoch 192/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4625 - acc: 0.7780 - val_loss: 0.6506 - val_acc: 0.6665\n",
      "Epoch 193/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4593 - acc: 0.7789 - val_loss: 0.6504 - val_acc: 0.6665\n",
      "Epoch 194/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4578 - acc: 0.7760 - val_loss: 0.6504 - val_acc: 0.6677\n",
      "Epoch 195/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4600 - acc: 0.7774 - val_loss: 0.6504 - val_acc: 0.6673\n",
      "Epoch 196/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4579 - acc: 0.7789 - val_loss: 0.6503 - val_acc: 0.6680\n",
      "Epoch 197/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4568 - acc: 0.7790 - val_loss: 0.6503 - val_acc: 0.6665\n",
      "Epoch 198/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 112ms/step - loss: 0.4592 - acc: 0.7790 - val_loss: 0.6502 - val_acc: 0.6665\n",
      "Epoch 199/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4593 - acc: 0.7751 - val_loss: 0.6502 - val_acc: 0.6673\n",
      "Epoch 200/200\n",
      "Learning rate:  5e-07\n",
      "94/93 [==============================] - 10s 111ms/step - loss: 0.4589 - acc: 0.7795 - val_loss: 0.6502 - val_acc: 0.6673\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "cnn_classifier_train = cnn_classifier.fit_generator(datagen.flow(x_train, labels_train, batch_size = 128),\n",
    "                epochs=epochs,steps_per_epoch=len(x_train) / 128,\n",
    "                shuffle=True,verbose = 1,\n",
    "                validation_data=(x_validation, labels_test), callbacks = callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "kyidIZgtTpqa",
    "outputId": "34b2e1be-43cc-4c23-b444-d71525f3ef04"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJu6GYRgNkP8HTPgz+gD9OBIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = cnn_classifier_train.history['acc']\n",
    "val_acc = cnn_classifier_train.history['val_acc']\n",
    "epochs = range(200)\n",
    "plt.figure()\n",
    "plt.plot(epochs, acc, 'go --', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aroGL4c-CL9M"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "cnn_classifier.save('/content/gdrive/My Drive/Chexnet/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XvlcxjHaEZKc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "Y_prediction = cnn_classifier.predict(x_validation)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_prediction,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(labels_test,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "FJ-ZqMHGFf52",
    "outputId": "80f2b32b-ece6-45c1-8ce3-fa9104ac887a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAikAAAHVCAYAAAA0K2vhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt=\"d\");"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ChexNET_CNNv3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
